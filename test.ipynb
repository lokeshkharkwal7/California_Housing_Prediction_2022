{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30000e+01, 2.30000e+01, 3.40000e+01, 4.42420e+04, 4.24242e+05,\n",
       "        2.32323e+05]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# using pickle file stored in tranformed obj to transform the test data\n",
    "import pickle\n",
    "file_path = ''\n",
    "with open(path , 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "data = [[23,23,34,44242,424242,232323,343434,323232]]\n",
    "obj.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://figshare.com/ndownloader/files/5976036'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from housing_project.keys.key import *\n",
    "URLKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to test.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define your configuration data as a Python dictionary\n",
    "config_data = {\n",
    "    'database': {\n",
    "        'host': 'localhost',\n",
    "        'port': 5432,\n",
    "        'username': 'myuser',\n",
    "        'password': 'mypassword',\n",
    "        'database_name': 'mydb',\n",
    "    },\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'file_path': '/var/log/myapp.log',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a YAML string\n",
    "yaml_string = yaml.dump(config_data, default_flow_style=False)\n",
    "\n",
    "# Specify the file path where you want to save the YAML configuration\n",
    "file_path = 'test.yaml'\n",
    "\n",
    "# Write the YAML string to the file\n",
    "with open(file_path, 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_string)\n",
    "\n",
    "print(f\"Configuration saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.util import  write_config\n",
    "data = { 'candidate_name_2':{\n",
    "    'name': 'Shantanu',\n",
    "    'cast': 'Sount',\n",
    "    'marks': 94     }\n",
    "}\n",
    "\n",
    "file_path = 'test.yaml'\n",
    "write_config(data , file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to edit data present in the config.yaml file ( first read and then update then push)\n",
    "from utility.util import  *\n",
    "# Creating an dict to update the config.yaml file\n",
    "data = { 'candidate_name_3' : {\n",
    "    'name': 'Pankaj',\n",
    "    'cast': 'Aggarwal',\n",
    "    'marks': 92\n",
    "}\n",
    "\n",
    "}\n",
    "# after reading the previous config.yaml file \n",
    "\n",
    "file_path = 'test.yaml'\n",
    "yaml_data = read_config(file_path)\n",
    "\n",
    "\n",
    "# using the update function update the dict we get from the read_config\n",
    "\n",
    "yaml_data.update(data)\n",
    "\n",
    "# pushing back the data to the config.yaml file\n",
    "write_config(yaml_data, file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the information that is present in the config.yaml file\n",
    "\n",
    "from utility.util import  *\n",
    "\n",
    "# reading the config.yaml file\n",
    "file_path = 'test.yaml'\n",
    "yaml_data = read_config(file_path)\n",
    "\n",
    "\n",
    "# updating the data with the information of the candidates with the highest score\n",
    "\n",
    "# highest scoreing info:\n",
    "\n",
    "highest_score = max([ i['marks'] for i in yaml_data.values()])\n",
    "higest_score_candidate = [ i for i in yaml_data.values()  if i.get('marks')==highest_score]\n",
    "yaml_data['best_candidate_info']['name'] = higest_score_candidate[0]['name']\n",
    "\n",
    "yaml_data['best_candidate_info']['cast'] = higest_score_candidate[0]['cast']\n",
    "\n",
    "yaml_data['best_candidate_info']['marks'] = higest_score_candidate[0]['marks']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pushing this list back to the config.yaml file\n",
    "\n",
    "write_config(yaml_data , file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_candidate_info': {'cast': 'Sount', 'marks': 94.9, 'name': 'Shantanu'},\n",
       " 'candidate_name_2': {'cast': 'Sount', 'marks': 94.9, 'name': 'Shantanu'},\n",
       " 'candidate_name_3': {'cast': 'Rana', 'marks': 94.1, 'name': 'Amit'},\n",
       " 'candidate_name_4': {'cast': 'Aggarwal', 'marks': 94.2, 'name': 'Pankaj'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80, 94.3, 94.2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i['marks'] for i in data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imfo = [ i for i in data.values() if i.get('marks') == max_marks]\n",
    "type(imfo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Best Model': {'Model_Score': 0, 'Model_name': '', 'Pickle_Locaton': '', 'best_model_parameters': ''}, 'Model03_09_00_25_31_': {'Model_name': 'RandomForestRegressor', 'Model_score': 0.7074716089658655, 'Pickle Locaton': 'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project\\\\Model_container\\\\01_09_11_56_33_\\\\GradientBoostingRegressor.pkl', 'best_model_parameters': {'max_depth': 10, 'n_estimators': 200}}, 'Model03_09_00_27_29_': {'Model_name': 'RandomForestRegressor', 'Model_score': 0.7074716089658655, 'Pickle Locaton': 'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project\\\\Model_container\\\\01_09_11_56_33_\\\\GradientBoostingRegressor.pkl', 'best_model_parameters': {'max_depth': 10, 'n_estimators': 200}}}\n"
     ]
    }
   ],
   "source": [
    "    from utility.util import *\n",
    "    file_path = 'model_status.yaml'\n",
    "    yaml_data = read_config(file_path)\n",
    "\n",
    "  \n",
    "\n",
    "    print(yaml_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # updating the data with the information of the candidates with the highest score\n",
    "\n",
    "    # highest scoreing info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # reading the config.yaml file\n",
    "from utility.util import  *\n",
    "file_path = 'model_status.yaml'\n",
    "yaml_data = read_config(file_path)\n",
    "\n",
    "\n",
    "    # updating the data with the information of the candidates with the highest score\n",
    "\n",
    "    # highest scoreing info:\n",
    "highest_score = max([ i['Model_score'] for i in yaml_data.values()])\n",
    "highest_score_model = [ i for i in yaml_data.values()  if i.get('Model_score')==highest_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project\\\\Model_container\\\\01_09_11_56_33_\\\\GradientBoostingRegressor.pkl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_info_to_yaml = { f'Model{datetime.now().strftime(\"%d_%m_%H_%M_%S_\")}':\n",
    "           { 'Model_name':'RandomForestRegressor',\n",
    "            'Model_score':0.7074716089658655,\n",
    "            'Pickle Locaton':'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project\\\\Model_container\\\\01_09_11_56_33_\\\\GradientBoostingRegressor.pkl',\n",
    "            'best_model_parameters':'best_model_param'}\n",
    "        }\n",
    "\n",
    " # Sending this information to yaml file\n",
    "model_entry_to_yaml(model_info_to_yaml)\n",
    "\n",
    "        # selecting the best model_from model_staus.yaml\n",
    "\n",
    "best_model_info = get_best_model_from_yaml()\n",
    "\n",
    "        # getting the pickle file from the best model:\n",
    "best_model_info['Pickle_Locaton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.74887756e+02,  1.57889395e+03, -4.82859560e-01,\n",
       "        -5.32101079e-02,  4.16736710e+02,  2.45830098e+00,\n",
       "        -8.50587599e-01,  7.74275012e+02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "Zscore_path = 'C:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project\\\\Z_Score_Obj\\\\Z_score.pkl'\n",
    "\n",
    "with open(Zscore_path, 'rb') as f:\n",
    "   model =  pickle.load(f)\n",
    "\n",
    "model.transform([[1234,3423,23,2323,123213, 3232,232,1244]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\Testing project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Logs\\\\log_2023-07-09-18_24_09.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "embedded null character",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_regression \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mtransformed_obj\u001b[39;49m\u001b[39m\\0\u001b[39;49;00m\u001b[39m8_09_17_20_26_\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mransformation.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m model_regression\u001b[39m.\u001b[39mfit_transform([[\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m1222\u001b[39m]])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: embedded null character"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_regression = pickle.load(open('transformed_obj\\08_09_17_20_26_\\transformation.pkl', 'rb'))\n",
    "\n",
    " \n",
    "\n",
    "model_regression.fit_transform([[12,12,12,12,12,12,12,1222]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66.58113951 -11.07389495  -1.41252678  -1.76440941  -1.76374209\n",
      "   -1.78782118  -1.76883302   5.23464915]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([236953.33163521])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "z_score_obj = 'Z_Score_Obj\\\\Z_score.pkl'\n",
    "\n",
    "with open(z_score_obj, 'rb') as f:\n",
    "    z_score = pickle.load(f)\n",
    "\n",
    "pca_obj = 'transformed_obj\\\\06_09_18_24_54_\\\\transformation.pkl'\n",
    "\n",
    "with open(pca_obj, 'rb') as f:\n",
    "    pca = pickle.load(f)\n",
    "\n",
    "\n",
    "model_obj = 'Best_model\\\\08_09_18_03_22_\\\\RandomForestRegressor.pkl'\n",
    "\n",
    "\n",
    "with open(model_obj, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "df = [[12,12,12,12,12,12,12,12]]\n",
    "z_transformed = z_score.transform(df)\n",
    "\n",
    "print(z_transformed)\n",
    "pca_transformed = pca.transform(z_transformed)\n",
    "model.predict(pca_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but RandomForestRegressor is expecting 6 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(p_model_obj, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mpredict(df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:508\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    507\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:519\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[39mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[39m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    518\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 519\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mpredict(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:984\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    982\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    986\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    987\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    600\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    627\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but RandomForestRegressor is expecting 6 features as input."
     ]
    }
   ],
   "source": [
    "p_model_obj = 'Final_Pipeline\\\\final_pipeline.pkl'\n",
    "\n",
    "\n",
    "with open(p_model_obj, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236953.33163521]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a custom transformer that combines Z-score, PCA, and model prediction\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, z_score_path, pca_path, model_path):\n",
    "        self.z_score_path = z_score_path\n",
    "        self.pca_path = pca_path\n",
    "        self.model_path = model_path\n",
    "        self.z_score = None\n",
    "        self.pca = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Load the Z-score and PCA objects\n",
    "        with open(self.z_score_path, 'rb') as f:\n",
    "            self.z_score = pickle.load(f)\n",
    "        \n",
    "        with open(self.pca_path, 'rb') as f:\n",
    "            self.pca = pickle.load(f)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.z_score is None or self.pca is None:\n",
    "            raise ValueError(\"Transformer not fitted\")\n",
    "        \n",
    "        # Z-score normalization\n",
    "        z_transformed = self.z_score.transform(X)\n",
    "\n",
    "        # PCA transformation\n",
    "        pca_transformed = self.pca.transform(z_transformed)\n",
    "        return pca_transformed\n",
    "\n",
    "# Load the model separately\n",
    "model_path = 'Best_model\\\\08_09_18_03_22_\\\\RandomForestRegressor.pkl'\n",
    "with open(model_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Create a pipeline with the custom transformer and the model\n",
    "pipeline = Pipeline([\n",
    "    ('custom_transformer', CustomTransformer(z_score_path='Z_Score_Obj\\\\Z_score.pkl',\n",
    "                                             pca_path='transformed_obj\\\\06_09_18_24_54_\\\\transformation.pkl',\n",
    "                                             model_path=model_path)),\n",
    "    ('model', model)  # Add the model as the final step\n",
    "])\n",
    "\n",
    "# Example data\n",
    "df = np.array([[12, 12, 12, 12, 12, 12, 12, 12]])\n",
    "\n",
    "# Fit the transformer before using it for transformation\n",
    "pipeline.named_steps['custom_transformer'].fit(df)\n",
    "\n",
    "# Make predictions using the pipeline\n",
    "predictions = pipeline.predict(df)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lokesh\\\\Desktop\\\\California Housing Price Prediction 2020'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_status.yaml', 'r') as file:\n",
    "            text_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'logs\\log_2023-09-09-02_54_53.log'\n",
    "with open(file , 'rb') as f:\n",
    "    output = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs\\\\log_2023-09-09-14_38_23.log'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_latest_file('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-09 14:38:30,354]^;INFO^;14^;__init__.py^;<module>()^;Pandas backend loaded 2.0.3\n",
      "[2023-09-09 14:38:30,382]^;INFO^;25^;__init__.py^;<module>()^;Numpy backend loaded 1.23.5\n",
      "[2023-09-09 14:38:30,382]^;INFO^;37^;__init__.py^;<module>()^;Pyspark backend NOT loaded\n",
      "[2023-09-09 14:38:30,382]^;INFO^;43^;__init__.py^;<module>()^;Python backend loaded\n",
      "[2023-09-09 14:38:32,832]^;WARNING^;187^;_internal.py^;_log()^; * Debugger is active!\n",
      "[2023-09-09 14:38:32,840]^;INFO^;187^;_internal.py^;_log()^; * Debugger PIN: 799-365-492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utility.util import  *\n",
    "op = get_latest_file('logs')\n",
    "with open(op , 'r') as f:\n",
    "    char = f.read()\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_generation import output_generation\n",
    " \n",
    "op= output_generation([[12,12,12,12,12,12,12,12]])\n",
    "print(op.generate_output())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.41556866  2.03789405  1.42184531 17.46569732 -0.74491098 10.68828395]]\n",
      "old z _score\n",
      "[[ 65.60787178 -11.02506827  -1.36623947  -1.65011343  -1.64288661\n",
      "   -1.67245689  -1.66435745   5.16299414]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npca_df = pca.transform(z_scoredf)\\noutput = model.predict(pca_df)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pickle\n",
    "z_score_loc = os.path.join(os.getcwd(), 'Pipeline Models and Transformers\\z_score_scaling.pkl')\n",
    "pca_loc  = os.path.join(os.getcwd(), 'Pipeline Models and Transformers\\pca.pkl')\n",
    "model_info  = os.path.join(os.getcwd(), 'Pipeline Models and Transformers\\Best_model.pkl')\n",
    "test_z_score = 'C:\\\\Users\\\\Lokesh\\\\Desktop\\\\California Housing Price Prediction 2020\\\\Z_Score_Obj\\\\Z_score.pkl'\n",
    "\n",
    "\n",
    "with open (z_score_loc , 'rb') as f:\n",
    "    z_score = pickle.load(f)\n",
    "\n",
    "\n",
    "with open (pca_loc , 'rb') as f:\n",
    "   pca =  pickle.load(f)\n",
    "\n",
    "\n",
    "with open (model_info , 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "with open (test_z_score , 'rb') as f:\n",
    "    z_test = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# creating a model from all of the files \n",
    "\n",
    "data = [[12,12,12,12,12,12,12,12]]\n",
    "z_scoredf = z_score.transform(data)\n",
    "print(z_scoredf)\n",
    "\n",
    "print('old z _score')\n",
    "df= z_test.transform(data)\n",
    "print(df)\n",
    "\n",
    "'''\n",
    "pca_df = pca.transform(z_scoredf)\n",
    "output = model.predict(pca_df)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z_score = 'C:\\\\Users\\\\Lokesh\\\\Desktop\\\\California Housing Price Prediction 2020\\\\Z_Score_Obj\\\\Z_score.pkl'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
